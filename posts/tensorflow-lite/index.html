<!DOCTYPE html>

<html lang="en-us">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="format-detection" content="telephone=no"/>

    <title>Zeyad Deeb</title>
    
    <link rel='icon' href='favicon.ico' type='image/x-icon'/>
    
    <meta name="theme-color" content="#ffffff">

    
    
    
    <link rel="stylesheet" href="https://www.zeyaddeeb.com/css/main.min.07a9cfa1a3a8999e9a21606064f4a147e568dd57c3010c09f068f6a9c274e9ca.css"/>

    
    
    

    
    
 
    
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-76463767-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

</head>
    <body>
        
<nav>
  <header>
    <div class="site-title">
        <a href="/">Zeyad Deeb</a>
    </div>  
</header>
  <div class="nav-menu">
  
    <a class="color-link nav-link" href="/about/">About</a>
  
  <a class="color-link nav-link" href="https://www.zeyaddeeb.com/index.xml" target="_blank" rel="noopener" type="application/rss+xml">RSS</a>
</div>
<footer class="footer">
	<div class="social-icons">
        

    

    

    

    

    

    

    

    

    

    
    <a class="social-icon" href="https://www.linkedin.com/in/zeyaddeeb" target="_blank" rel="noopener" title="LinkedIn">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <path d="M2,3.654102 C2,2.69908141 2.79442509,1.92397846 3.77383592,1.92397846 L24.2261641,1.92397846 C25.2058917,1.92397846 26,2.69908141 26,3.654102 L26,24.3462148 C26,25.3015521 25.2058917,26.0760215 24.2261641,26.0760215 L3.77383592,26.0760215 C2.79442509,26.0760215 2,25.3015521 2,24.3465315 L2,3.65378524 L2,3.654102 Z M9.27526132,22.1415901 L9.27526132,11.2356668 L5.65030092,11.2356668 L5.65030092,22.1415901 L9.27557808,22.1415901 L9.27526132,22.1415901 Z M7.46341463,9.74691162 C8.72727273,9.74691162 9.51409566,8.90940767 9.51409566,7.86284447 C9.49033893,6.79252455 8.72727273,5.97846056 7.48748812,5.97846056 C6.24675325,5.97846056 5.43649034,6.79252455 5.43649034,7.86284447 C5.43649034,8.90940767 6.22299652,9.74691162 7.4396579,9.74691162 L7.46309788,9.74691162 L7.46341463,9.74691162 Z M11.2815965,22.1415901 L14.9062401,22.1415901 L14.9062401,16.0519481 C14.9062401,15.7263225 14.9299968,15.4000634 15.0256573,15.1675641 C15.2876148,14.5159962 15.8840672,13.8416218 16.8856509,13.8416218 C18.1970225,13.8416218 18.7218879,14.8416218 18.7218879,16.3078872 L18.7218879,22.1415901 L22.3465315,22.1415901 L22.3465315,15.8885017 C22.3465315,12.5388027 20.5584416,10.9800443 18.1735825,10.9800443 C16.2182452,10.9800443 15.3595185,12.072854 14.8824834,12.8172315 L14.9065569,12.8172315 L14.9065569,11.2359835 L11.2819132,11.2359835 C11.3291099,12.2591067 11.2815965,22.1419069 11.2815965,22.1419069 L11.2815965,22.1415901 Z"></path>
        </svg>
    </a>
    

    

    

    

    

    

    

    
    
    
    <a class="social-icon" href="https://github.com/zeyaddeeb" target="_blank" rel="noopener" title="GitHub">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <path d="M13.9988029,1.32087331 C6.82105037,1.32087331 1,7.14112562 1,14.3212723 C1,20.0649109 4.72454649,24.9370678 9.89038951,26.6560892 C10.5408085,26.7757983 10.7778323,26.374374 10.7778323,26.0296121 C10.7778323,25.7215609 10.7666595,24.9035493 10.760275,23.8189856 C7.14426471,24.6042767 6.38131925,22.0760223 6.38131925,22.0760223 C5.78995672,20.5740732 4.93762853,20.1742451 4.93762853,20.1742451 C3.75729765,19.3682044 5.02701126,19.3841656 5.02701126,19.3841656 C6.33183953,19.4759425 7.01817121,20.7241085 7.01817121,20.7241085 C8.17775254,22.7104801 10.0611744,22.1366749 10.8017741,21.8038838 C10.919887,20.9643246 11.2558703,20.3913175 11.6269683,20.066507 C8.74038491,19.7385043 5.70536235,18.6228163 5.70536235,13.6413251 C5.70536235,12.2223743 6.21213051,11.0611968 7.04370914,10.1530044 C6.90963504,9.82420367 6.46351945,8.50181809 7.17139875,6.71256734 C7.17139875,6.71256734 8.26234691,6.36301702 10.7459099,8.04532771 C11.78259,7.75642995 12.8950858,7.61277914 14.000399,7.60719272 C15.1049142,7.61277914 16.2166119,7.75642995 17.2548881,8.04532771 C19.736855,6.36301702 20.8262071,6.71256734 20.8262071,6.71256734 C21.5356825,8.50181809 21.0895669,9.82420367 20.9562909,10.1530044 C21.7894656,11.0611968 22.2922435,12.2223743 22.2922435,13.6413251 C22.2922435,18.6355852 19.2524325,19.734514 16.3570705,20.0561322 C16.8231376,20.4575564 17.2389269,21.2508282 17.2389269,22.4638795 C17.2389269,24.2012564 17.2229657,25.603448 17.2229657,26.0296121 C17.2229657,26.3775663 17.4575954,26.7821827 18.116793,26.6552912 C23.2786458,24.9322794 27,20.0633148 27,14.3212723 C27,7.14112562 21.1789496,1.32087331 13.9988029,1.32087331"></path>
        </svg>
    </a>
    

    
    
    

    
</div>




	<script src="https://www.zeyaddeeb.com/js/main.min.fa5c2b23e07b5d9bfad267a52b4b24fdb053e6fb7524993383594926a3ac270c.js" integrity="sha256-+lwrI+B7XZv60melK0sk/bBT5vt1JJkzg1lJJqOsJww="></script>
</footer>
</nav>
        <div id="content" class="content-container">
        
<h1 class="post-title">TensorFlow Lite: Adventures in Scaling Production Models</h1>
    <time>December 12, 2020</time>
    <div>
        <p>
        <p>As a Data Scientist, I not only need to know how to develop and train models, I also need to deploy them to production to actually perform a their intended function, be it add face filter on your silly video (think: Snapchat), or a personal assistant which answers questions fo you (think: Siri).</p>
<p>For a long time, the tool I most regularly used for building deep learning models is TensorFlow. I occasionally got questions about why I don't use Pytorch instead, since it's newer and requires less effort. The answer is that I have actually used it, even since the time when it was recommended for &ldquo;Research Use Only.&rdquo; Back then, it caused quite a commotion with dynamic graphs! That took some getting used to. Fo all intents and purposes, it'd been easier for me to stick with TensorFlow. As it happens, when TensorFlow 2 came out it, too, used dynamic graphs, and at that point I felt I just had to get on board. Opting to stick with TensorFlow has been a <a href="https://en.wikipedia.org/wiki/Path_of_least_resistance">Path of Least Resistance</a> tactic, nothing more.</p>
<p>Anyhow, this is not a Tensorflow vs. Pytorch comparison. There are already way too many Medium articles which lay out all the differences. For now, my main interest is in introducing you to <a href="https://www.tensorflow.org/lite">Tensorflow Lite</a>.</p>
<p>From the authors:</p>
<blockquote>
<p>TensorFlow Lite is an open source deep learning framework for on-device inference. Works in 4 Steps:</p>
<ul>
<li>Pick a model: Pick a new model or retrain an existing one.</li>
<li>Convert: Convert a TensorFlow model into a compressed flat buffer with the TensorFlow Lite Converter.</li>
<li>Deploy: Take the compressed .tflite file and load it into a mobile or embedded device.</li>
<li>Optimize: Quantize by converting 32-bit floats to more efficient 8-bit integers or run on GPU.</li>
</ul>
</blockquote>
<p>To summarize, you can take an existing <code>model.tf</code> file and convert it to <code>model.tflite</code> which gives you added benefits like reducing file size (using quantization) and making it compatible with mobile devices. It's currently true that not all models can be converted, since some deep learning graph operations are not yet supported. Still, it's been my experience that they typically the core Tensorflow development team adds new operations pretty quickly. Personally, all my model conversions needs can be met with <code>Tensorflow 2.4.0-rc1</code>)</p>
<p>As with everything, there are alternatives to TensorFlow Lite when attempting to deploy production models. For example, you could serve the model with <a href="https://www.tensorflow.org/tfx">TFX</a> as a webserver. While this is good, it leaves you without the ability to do event-driven predictions (Kakfa enrichment, for example). At that point, there's still the potential that you'd run into all the common issues one tends to have with webservers, like failures due to networking issues, timeouts, high workloads, etc&hellip; Apart from its mobile and embedded devices perks, Tensorflow Lite's file size reduction is very advantageous when you have an autoscaled model deployment, like an <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale">HPA in Kubernetes</a>, which dynamically increases the number of available pods when you have higher workloads. When it comes to file size, you have two options: either you use mountable volumes, which can be only used by one node at a time, or you require the model be downloaded every time you auto scale. Imagine having to download 2, 3 or 10 gigs at a time, every time you spin up a new instance. It's so inefficient!</p>
<p>The first step to using TensorFlow Lite is model conversion (turning that <code>.tf</code> file into a <code>.tflite</code>), but I'll skip this step here since it requires another blog post all together. You can find more details on how to convert your regular Tensorflow model to Tensorflow Lite <a href="https://www.tensorflow.org/lite/convert">here</a>. My main interest is discussing the specifics of deploying the model using Tensorflow Lite.</p>
<p>Second, which is the core benefit of using TensorFlow Lite is the ability to ship your models to iOS and Android devices. If you read the documentation closely, you will notice that the whole API was designed to serve that purpose. Take the <code>.invoke()</code> method after you load the interpreter:</p>
<p>A TensorFlow Lite model in Python might look like</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">tensorflow</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">tf</span>

<span style="color:#007020;font-weight:bold">class</span> <span style="color:#0e84b5;font-weight:bold">LukeSkywalker</span>:
    <span style="color:#007020;font-weight:bold">def</span> __init__(self, model_path: <span style="color:#007020">str</span> <span style="color:#666">=</span> <span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">model.tflite</span><span style="color:#4070a0">&#34;</span>) <span style="color:#666">-</span><span style="color:#666">&gt;</span> None:
        self<span style="color:#666">.</span>interpreter <span style="color:#666">=</span> tf<span style="color:#666">.</span>lite<span style="color:#666">.</span>Interpreter(model_path)
        self<span style="color:#666">.</span>interpreter<span style="color:#666">.</span>allocate_tensors()
        self<span style="color:#666">.</span>input_details <span style="color:#666">=</span> self<span style="color:#666">.</span>interpreter<span style="color:#666">.</span>get_input_details()
        self<span style="color:#666">.</span>output_details <span style="color:#666">=</span> self<span style="color:#666">.</span>interpreter<span style="color:#666">.</span>get_output_details()

    <span style="color:#007020;font-weight:bold">def</span> <span style="color:#06287e">predict</span>(self, input_data: np<span style="color:#666">.</span>array) <span style="color:#666">-</span><span style="color:#666">&gt;</span> np<span style="color:#666">.</span>array:
        self<span style="color:#666">.</span>interpreter<span style="color:#666">.</span>set_tensor(self<span style="color:#666">.</span>input_details[<span style="color:#40a070">0</span>][<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">index</span><span style="color:#4070a0">&#34;</span>], input_data)
        self<span style="color:#666">.</span>interpreter<span style="color:#666">.</span>invoke()
        <span style="color:#007020;font-weight:bold">return</span> np<span style="color:#666">.</span>squeeze(self<span style="color:#666">.</span>interpreter<span style="color:#666">.</span>get_tensor(self<span style="color:#666">.</span>output_details[<span style="color:#40a070">0</span>][<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">index</span><span style="color:#4070a0">&#34;</span>])[<span style="color:#40a070">0</span>])</code></pre></div>
<p>The Python example is pretty straightforward and concise. If you wanted to do it in Swift, you'd do whatever those guys do with MVC patterns:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-swift" data-lang="swift"><span style="color:#007020;font-weight:bold">init</span>?(modelFileInfo: FileInfo, labelsFileInfo: FileInfo, threadCount: <span style="color:#007020">Int</span> = <span style="color:#40a070">1</span>) {
 ...
  <span style="color:#007020;font-weight:bold">self</span>.threadCount = threadCount
  <span style="color:#007020;font-weight:bold">var</span> <span style="color:#bb60d5">options</span> = InterpreterOptions()
  options.threadCount = threadCount
  options.isErrorLoggingEnabled = <span style="color:#007020;font-weight:bold">true</span>
  <span style="color:#007020;font-weight:bold">do</span> {
    interpreter = <span style="color:#007020;font-weight:bold">try</span> Interpreter(modelPath: modelPath, options: options)
  } <span style="color:#007020;font-weight:bold">catch</span> <span style="color:#007020;font-weight:bold">let</span> <span style="color:#bb60d5">error</span> {
    print(<span style="color:#4070a0">&#34;</span><span style="color:#4070a0">Failed to create the interpreter with error: </span><span style="color:#70a0d0;font-style:italic">\(</span>error.localizedDescription<span style="color:#70a0d0;font-style:italic">)</span><span style="color:#4070a0">&#34;</span>)
    <span style="color:#007020;font-weight:bold">return</span> <span style="color:#007020;font-weight:bold">nil</span>
  }</code></pre></div>
<p>Notice that you can specify thread count for the model and you can also delegate to <a href="https://www.tensorflow.org/lite/performance/coreml_delegate">CoreML</a> if you have that enabled in your Swift project.</p>
<p>The main caveat here is that Tensorflow Lite is threaded by default and you need to either define the number of threads you want to use, or just set it to <code>0</code>. But even then you can run into another problem if you don't know what resources are available to you. For example, I use spot instances pools on AWS and those are varied, with different kinds of Intel processors, which makes it hard to know if I should thread, and if so, how much.</p>
<p>Let's try it anyways, an example of setting the number of threads in python:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">os</span>

<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">tensorflow</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">tf</span>

os<span style="color:#666">.</span>environ[<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">TF_NUM_INTEROP_THREADS</span><span style="color:#4070a0">&#34;</span>] <span style="color:#666">=</span> <span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">1</span><span style="color:#4070a0">&#34;</span>
os<span style="color:#666">.</span>environ[<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">TF_NUM_INTRAOP_THREADS</span><span style="color:#4070a0">&#34;</span>] <span style="color:#666">=</span> <span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">4</span><span style="color:#4070a0">&#34;</span>

<span style="color:#007020;font-weight:bold">class</span> <span style="color:#0e84b5;font-weight:bold">LukeSkywalker</span>:
<span style="color:#666">.</span><span style="color:#666">.</span><span style="color:#666">.</span></code></pre></div>
<p>To give some more context around the purpose of these environment variables:</p>
<p><code>TF_NUM_INTEROP_THREADS</code>: Determines the number of threads used by independent non-blocking operations. 0 means the system picks an appropriate number.</p>
<p><code>TF_NUM_INTRAOP_THREADS</code>: Certain operations like matrix multiplication and reductions can utilize parallel threads for speed ups. A value of 0 means the system picks an appropriate number.</p>
<p>I spent an entire week of my life trying to fine tune an &ldquo;appropriate number&rdquo; where I could maintain high performance. While researching recommendations from the Tensorflow community, everyone said the same thing: always set both inter and intraop thread counts to <code>1</code>. This <a href="https://djl.ai/docs/development/inference_performance_optimization.html"><code>djl</code></a> library is just one of many places I saw the recommendation.
I found this really frustrating. If you benchmark for model speed, we are talking in some cases <code>700 milliseconds</code> per inference when <code>TF_NUM_INTRAOP_THREADS=1</code> vs <code>200 milliseconds</code> per inference when <code>TF_NUM_INTRAOP_THREADS=4</code> (This will vary by what kind of model). So why in the world would anyone just use <code>1</code>?</p>
<p>So I ignored the recommendation, and I decided to test my model on a 4-node kubernetes cluster and set <code>TF_NUM_INTRAOP_THREADS</code> to <code>4</code>. What I quickly noticed was the nodes started running at <code>400%</code> CPU utilization, which I attributed to AWS using a standard commodity <code>t3.xlarge</code> 4-core machine. Then I ran into an even bigger problem, since the HPA autoscaling I'd set up got triggered, thus creating 3 more pods with the same model, all running at that same high utilization rate. This left me with four pods, all running at 400% utilization, which result cannibalization in system resources. This was particularly distressing, since I'd started this whole endeavor thinking I was going to get faster results!</p>
<p>Being the crafty Python developer that I am, I decided to try setting my thread count to <code>TF_NUM_INTRAOP_THREADS=1</code> while also using <code>ThreadPoolExecutor</code>, maybe that will help (GIL, says no)</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">concurrent.futures</span>
<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">os</span>

<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">tensorflow</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">tf</span>

os<span style="color:#666">.</span>environ[<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">TF_NUM_INTEROP_THREADS</span><span style="color:#4070a0">&#34;</span>] <span style="color:#666">=</span> <span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">1</span><span style="color:#4070a0">&#34;</span>
os<span style="color:#666">.</span>environ[<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">TF_NUM_INTRAOP_THREADS</span><span style="color:#4070a0">&#34;</span>] <span style="color:#666">=</span> <span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">1</span><span style="color:#4070a0">&#34;</span>

<span style="color:#007020;font-weight:bold">class</span> <span style="color:#0e84b5;font-weight:bold">LukeSkywalker</span>:
<span style="color:#666">.</span><span style="color:#666">.</span><span style="color:#666">.</span>

<span style="color:#007020;font-weight:bold">if</span> __name__ <span style="color:#666">==</span> <span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">__main__</span><span style="color:#4070a0">&#34;</span>:
    predictor <span style="color:#666">=</span> LukeSkywalker()
    <span style="color:#007020;font-weight:bold">with</span> concurrent<span style="color:#666">.</span>futures<span style="color:#666">.</span>ThreadPoolExecutor(max_workers<span style="color:#666">=</span><span style="color:#40a070">4</span>) <span style="color:#007020;font-weight:bold">as</span> executor:
        future <span style="color:#666">=</span> executor<span style="color:#666">.</span>map(
            predictor<span style="color:#666">.</span>predict, [np<span style="color:#666">.</span>array([<span style="color:#40a070">0</span>, <span style="color:#40a070">0</span>, <span style="color:#40a070">0</span>]), np<span style="color:#666">.</span>array([<span style="color:#40a070">0</span>, <span style="color:#40a070">0</span>, <span style="color:#40a070">0</span>])]
        )

    <span style="color:#007020;font-weight:bold">print</span>(future<span style="color:#666">.</span>result())</code></pre></div>
<p>This resulted in thread-safe errors, because you can't share <code>.invoke()</code> methods across threads. Another dead end! Time to reverse course to a different approach for a different blog post.</p>
<p>And so goes the cautionary tale of using TensorFlow Lite to deploy your models to production.
Hopefully you now know a bit more about the scalability issues you can run into, and know how to avoid them.</p>
<p>You can build the best model man has ever seen, but if you can't scale it properly so it can be utilized in a timely manner, the model becomes obsolete.
Instead of fighting with thread counts, invest some time thinking about how your model is going to evolve over time, and what scale requirements you have before saying &ldquo;I'm going to use BERT with <code>max_token_length=1024</code>.&rdquo;</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://www.tensorflow.org/lite">https://www.tensorflow.org/lite</a></li>
<li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale</a></li>
</ul>

        </p>
    </div>
    

    <div class="page-footer">
        <hr class="footer-divider">
        
    </div>


        

<link rel="stylesheet" type="text/css" href="/css/katex.min.css">
<script type="text/javascript" src="/js/katex.min.js"></script>
<script type="text/javascript" src="/js/auto-render.min.js"onload="renderMathInElement(document.body);"></script>

        </div>
        <footer class="footer-mobile">
	<div class="social-icons">
        

    

    

    

    

    

    

    

    

    

    
    <a class="social-icon" href="https://www.linkedin.com/in/zeyaddeeb" target="_blank" rel="noopener" title="LinkedIn">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <path d="M2,3.654102 C2,2.69908141 2.79442509,1.92397846 3.77383592,1.92397846 L24.2261641,1.92397846 C25.2058917,1.92397846 26,2.69908141 26,3.654102 L26,24.3462148 C26,25.3015521 25.2058917,26.0760215 24.2261641,26.0760215 L3.77383592,26.0760215 C2.79442509,26.0760215 2,25.3015521 2,24.3465315 L2,3.65378524 L2,3.654102 Z M9.27526132,22.1415901 L9.27526132,11.2356668 L5.65030092,11.2356668 L5.65030092,22.1415901 L9.27557808,22.1415901 L9.27526132,22.1415901 Z M7.46341463,9.74691162 C8.72727273,9.74691162 9.51409566,8.90940767 9.51409566,7.86284447 C9.49033893,6.79252455 8.72727273,5.97846056 7.48748812,5.97846056 C6.24675325,5.97846056 5.43649034,6.79252455 5.43649034,7.86284447 C5.43649034,8.90940767 6.22299652,9.74691162 7.4396579,9.74691162 L7.46309788,9.74691162 L7.46341463,9.74691162 Z M11.2815965,22.1415901 L14.9062401,22.1415901 L14.9062401,16.0519481 C14.9062401,15.7263225 14.9299968,15.4000634 15.0256573,15.1675641 C15.2876148,14.5159962 15.8840672,13.8416218 16.8856509,13.8416218 C18.1970225,13.8416218 18.7218879,14.8416218 18.7218879,16.3078872 L18.7218879,22.1415901 L22.3465315,22.1415901 L22.3465315,15.8885017 C22.3465315,12.5388027 20.5584416,10.9800443 18.1735825,10.9800443 C16.2182452,10.9800443 15.3595185,12.072854 14.8824834,12.8172315 L14.9065569,12.8172315 L14.9065569,11.2359835 L11.2819132,11.2359835 C11.3291099,12.2591067 11.2815965,22.1419069 11.2815965,22.1419069 L11.2815965,22.1415901 Z"></path>
        </svg>
    </a>
    

    

    

    

    

    

    

    
    
    
    <a class="social-icon" href="https://github.com/zeyaddeeb" target="_blank" rel="noopener" title="GitHub">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <path d="M13.9988029,1.32087331 C6.82105037,1.32087331 1,7.14112562 1,14.3212723 C1,20.0649109 4.72454649,24.9370678 9.89038951,26.6560892 C10.5408085,26.7757983 10.7778323,26.374374 10.7778323,26.0296121 C10.7778323,25.7215609 10.7666595,24.9035493 10.760275,23.8189856 C7.14426471,24.6042767 6.38131925,22.0760223 6.38131925,22.0760223 C5.78995672,20.5740732 4.93762853,20.1742451 4.93762853,20.1742451 C3.75729765,19.3682044 5.02701126,19.3841656 5.02701126,19.3841656 C6.33183953,19.4759425 7.01817121,20.7241085 7.01817121,20.7241085 C8.17775254,22.7104801 10.0611744,22.1366749 10.8017741,21.8038838 C10.919887,20.9643246 11.2558703,20.3913175 11.6269683,20.066507 C8.74038491,19.7385043 5.70536235,18.6228163 5.70536235,13.6413251 C5.70536235,12.2223743 6.21213051,11.0611968 7.04370914,10.1530044 C6.90963504,9.82420367 6.46351945,8.50181809 7.17139875,6.71256734 C7.17139875,6.71256734 8.26234691,6.36301702 10.7459099,8.04532771 C11.78259,7.75642995 12.8950858,7.61277914 14.000399,7.60719272 C15.1049142,7.61277914 16.2166119,7.75642995 17.2548881,8.04532771 C19.736855,6.36301702 20.8262071,6.71256734 20.8262071,6.71256734 C21.5356825,8.50181809 21.0895669,9.82420367 20.9562909,10.1530044 C21.7894656,11.0611968 22.2922435,12.2223743 22.2922435,13.6413251 C22.2922435,18.6355852 19.2524325,19.734514 16.3570705,20.0561322 C16.8231376,20.4575564 17.2389269,21.2508282 17.2389269,22.4638795 C17.2389269,24.2012564 17.2229657,25.603448 17.2229657,26.0296121 C17.2229657,26.3775663 17.4575954,26.7821827 18.116793,26.6552912 C23.2786458,24.9322794 27,20.0633148 27,14.3212723 C27,7.14112562 21.1789496,1.32087331 13.9988029,1.32087331"></path>
        </svg>
    </a>
    

    
    
    

    
</div>





	<script src="https://www.zeyaddeeb.com/js/main.min.fa5c2b23e07b5d9bfad267a52b4b24fdb053e6fb7524993383594926a3ac270c.js" integrity="sha256-+lwrI+B7XZv60melK0sk/bBT5vt1JJkzg1lJJqOsJww="></script>
</footer>
    </body>
</html>